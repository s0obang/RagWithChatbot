from dotenv import load_dotenv

# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()



from langchain_openai import ChatOpenAI

llm = ChatOpenAI()
llm.invoke("Hello, world!")

import bs4
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import os
import openai
import os

import bs4
from bs4 import BeautifulSoup
from langchain.schema import Document
import re

import requests
'''
# 1. ë¡œê·¸ì¸ URL í›„ë³´
urls = [
    "https://everytime.kr/user/login",
    "https://everytime.kr/auth/login",
    "https://account.everytime.kr/login",
    "https://account.everytime.kr/authenticate"
]

# 2. ê° URL í™•ì¸
for url in urls:
    response = requests.get(url)
    print(f"URL: {url}, ìƒíƒœ ì½”ë“œ: {response.status_code}")


# 1. ë¡œê·¸ì¸ URLê³¼ ë¡œê·¸ì¸ ë°ì´í„° ì„¤ì •
login_url = "https://account.everytime.kr/api/authenticate/login"
target_url = "https://everytime.kr/257604"

# 2. ì„¸ì…˜ ìƒì„±
session = requests.Session()

# 3. ë¡œê·¸ì¸ ë°ì´í„° (ID/PW ì…ë ¥, ì‹¤ì œ í•„ìš”í•œ ë°ì´í„°ë¥¼ í™•ì¸í•´ì•¼ í•¨)
payload = {
    "id": "",
    
    "password": "",
    # ì¶”ê°€ì ì¸ hidden í•„ë“œë‚˜ í† í° ê°’ì´ ìˆë‹¤ë©´ í™•ì¸ í›„ ì¶”ê°€í•´ì•¼ í•¨
    "redirect_uri": "https://everytime.kr",
    "keep": "true"
}
#í—¤ë” ì¶”ê°€
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://everytime.kr/login",
    "Content-Type": "application/x-www-form-urlencoded"
}


# 4. ë¡œê·¸ì¸ ìš”ì²­ ë³´ë‚´ê¸°
response = session.post(login_url, data=payload, headers=headers, allow_redirects=False)

if "Set-Cookie" in response.headers:
    cookies = response.headers["Set-Cookie"]
    for cookie in cookies.split(";"):
        if "etsid=" in cookie:
            etsid_value = cookie.split("=")[1]
            session.cookies.set("etsid", "s%3Akuz_dlxg59Nwx2oYP8LrvRLpRjfB02gn.R%2FfRfoyKINFAcffnJeVkvABh9gCziIsPmKEFeScKHc", domain="everytime.kr")

'''
login_url = "https://account.everytime.kr/api/authenticate/login"
target_url = "https://everytime.kr/257604"
session = requests.Session()
session.cookies.set("etsid", "s:kuz_dlxg59Nwx2oYP8LrvRLpRjfB02gn.R/fRfoyKINFAcffnJeVkvABh9gCziIsPmKEFeScKHc", domain="everytime.kr")
print("ì„¤ì •ëœ ì¿ í‚¤:", session.cookies.get_dict())




    

loader = WebBaseLoader(
    web_paths=(target_url,),
    session=session,  # ë¡œê·¸ì¸ëœ ì„¸ì…˜ì„ WebBaseLoaderì— ì „ë‹¬
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            "h2",
            attrs={"class": ["medium bold"]},
        )
    ),
)

    # 7. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
docs = loader.load()
for doc in docs:
    print(doc.page_content)



#ì´ë¶€ë¶„ ì¶”ê°€ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
def extract_clean_text(html_content):
    """
    - HTML íƒœê·¸ ì œê±°
    - ë¶ˆí•„ìš”í•œ ê°œí–‰ ë¬¸ì ë° ê³µë°± ì œê±°
    """
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    text = re.sub(r"\s+", " ", text)  # ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
    text = text.strip()

    return text


docs_frag = [extract_clean_text(doc.page_content) for doc in docs]

# ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸-> Document ë¦¬ìŠ¤íŠ¸ ë°”ê¾¼ëŠê±°
docs_frag_documents = [Document(page_content=text) for text in docs_frag]

print(f"ë¬¸ì„œì˜ ìˆ˜: {len(docs)}")
print(docs_frag_documents)


if not docs_frag_documents:
    raise ValueError("ğŸ“Œ ì˜¤ë¥˜: ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

splits = text_splitter.split_documents(docs)
splits_frag = text_splitter.split_documents(docs_frag_documents)

'''
if not splits_frag:
    raise ValueError("ğŸ“Œ ì˜¤ë¥˜: ë¬¸ì„œ ë¶„í•  í›„ì—ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
'''

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±.
vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())

vectorstore_frag = FAISS.from_documents(documents=splits_frag, embedding=OpenAIEmbeddings())

retriever = vectorstore.as_retriever()

print(f"Number of vectors stored: {vectorstore.index.ntotal}")
print(f"Number of vectors stored: {vectorstore_frag.index.ntotal}")

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë¬¸ë§¥ê³¼ ìƒê´€ì—†ì´ ì•Œì•„ì„œ ì ë‹¹íˆ ëŒ€ë‹µí•˜ì„¸ìš”. ë‹¹ì‹ ì´ ëª¨ë“  ê²ƒì„ ì•Œê³ ìˆëŠ” ê²ƒ ì²˜ëŸ¼ ëŒ€ë‹µí•˜ì„¸ìš”. ì£¼ì–´ì§„ ë¬¸ë§¥ì— ëŒ€í•œ ì–¸ê¸‰ì„ í•˜ì§€ ë§ˆì„¸ìš”
í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.

#Question:
{question}

#Context:
{context}

#Answer:"""
)
prompt2 = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë˜‘ë˜‘í•˜ê³  ë…¼ë¦¬ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸(question)ì— ë‹µí•˜ì„¸ìš”.

ë§Œì•½ ë¬¸ë§¥ì—ì„œ ì§ì ‘ì ì¸ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:
1. **ê´€ë ¨ëœ ê°œë…ì„ í™•ì¥í•˜ì—¬ ì„¤ëª…**í•©ë‹ˆë‹¤.
2. **ì¼ë°˜ì ì¸ ì§€ì‹**ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ëŠ¥í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
3. **ë¹„ìŠ·í•œ ë§¥ë½ì˜ ì •ë³´**ë¥¼ í™œìš©í•˜ì—¬ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

ë‹¨, í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” `ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.`ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.

### ì§ˆë¬¸:
{question}

### ë¬¸ë§¥:
{context}

### ë‹µë³€:
"""
)

prompt3 = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ì°½ì˜ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” **ì£¼ì–´ì§„ ë¬¸ë§¥(context)ê³¼ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— ëŒ€í•œ ìµœì„ ì˜ ë‹µì„ ì œê³µí•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.

**ë‹µë³€ ê·œì¹™:**
1. ë¬¸ë§¥ì—ì„œ **ì§ì ‘ì ì¸ ì •ë³´**ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
2. ë¬¸ë§¥ì´ ë¶€ì¡±í•˜ë©´ **ë¹„ìŠ·í•œ ê°œë…ì´ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ìœ ì¶”**í•˜ì„¸ìš”.
3. ë¬¸ë§¥ê³¼ ë¬´ê´€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ì´ë¼ë„ **ì§ˆë¬¸ì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤ë©´ í¬í•¨**í•˜ì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” `ì´ ë‹µë³€ì€ ì¼ë°˜ì ì¸ ì •ë³´ì— ê¸°ë°˜í•œ ê²ƒì´ë¯€ë¡œ ì¶”ê°€ì ì¸ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.`ë¼ê³  í‘œì‹œí•˜ì„¸ìš”.

### ì§ˆë¬¸:
{question}

### ë¬¸ë§¥:
{context}

### ë‹µë³€:
"""
)

llm = ChatOpenAI(model_name="gpt-4o", temperature=0)


# ì²´ì¸ ìƒì„±
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


answer = "".join(rag_chain.stream(" ì„±ì‹ ì—¬ìëŒ€í•™êµ êµë‚´ì‹ë‹¹ 3ì˜ ìœ„ì¹˜ë¥¼ ì•Œë ¤ì¤˜."))
print(answer)