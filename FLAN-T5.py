from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import Prompt
from langchain.schema import Document
import os
import json
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline


def write(resultPath, resultSet):
    #ê¸°ì¡´ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° (ìˆìœ¼ë©´ ê¸°ì¡´ ë‚´ìš© ìœ ì§€)
    if os.path.exists(resultPath):
        with open(resultPath, "r", encoding="utf-8") as f:
            existing_data = f.read()
    else:
        existing_data = ""

    #ìƒˆë¡œìš´ ë°ì´í„° ì¶”ê°€
    with open(resultPath, "w", encoding="utf-8") as f:
        f.write(existing_data)
        for line in resultSet:
            f.write(line)

    print(f"ë³€í™˜ ì™„ë£Œ, ê²°ê³¼ ì €ì¥ë¨: {resultPath}")

def test(questionPath, resultPath):
    # ì§ˆë¬¸ íŒŒì¼ ì½ê¸°(ê³µë°± ì œê±°,í•œ ì¤„ì”© ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥)
    with open(questionPath, "r", encoding="utf-8") as f:
        questions = [line.strip() for line in f if line.strip()]  #ë¹ˆ ì¤„ ì œê±°

    results = []

    for idx, query in enumerate(questions, start=1):
        print(f"[{idx}] ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {query}")
        answer = "".join(rag_chain.stream(query))
        result_entry = f"[ì§ˆë¬¸ {idx}]\n{query}\n\n[ë‹µë³€]\n{answer}\n\n{'='*50}\n\n"
        results.append(result_entry)

    # ê²°ê³¼ ì €ì¥
    write(resultPath, results)

def load_json_to_documents(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    documents = []
    for item in data:
        title = item.get("title", "")
        content = item.get("content", "")
        comments = "\n".join(item.get("comments", []))
        create = item.get("create", "")

        full_text = f"{title}\n\n{content}\n\në‹µë³€:\n{comments}"
        documents.append(Document(page_content=full_text, metadata={"create": create}))
    
    return documents

# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()


input_file = "/Users/minseon/2025/í•™ë¶€ì—°êµ¬ìƒ/RAG/RagWithChatbot/output_data.json" 
#input_file = r"C:\Soop\ì—°êµ¬\RagTest\ChatBotWithRag\output_data.json" 
#input_file = "/Users/soop/s0obang/í•™ë¶€ì—°êµ¬ìƒ24w/RagWithChatbot/output_data.json"

docs = load_json_to_documents(input_file)

# í…ìŠ¤íŠ¸ë¥¼ ì²­í¬(Chunk)ë¡œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
splits = text_splitter.split_documents(docs)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (FAISS + OpenAI Embeddings)
vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever(search_kwargs={"k": 1})  #ì…ë ¥ í† í° í„°ì¡Œì„ ë•Œ ê°€ì ¸ì˜¤ëŠ” ë¬¸ì„œ ìˆ˜ ì¤„ì´ë ¤ë©´ íŒŒë¼ë¯¸í„°ë¡œ search_kwargs={"k": 3}

#prompt = Prompt.Prompt.prompt7
prompt = Prompt.Prompt.prompt_bart

'''
#bart
bart_pipe = pipeline(
    "text2text-generation",
    model="facebook/bart-large",
    tokenizer="facebook/bart-large",
    max_new_tokens=300,  # ì¶œë ¥ ê¸¸ì´ ì¡°ì •
    device=-1
)
'''

kobart_pipe = pipeline(
    "text2text-generation",
    model="digit82/kobart-summarization",  # ë˜ëŠ” ë‹¤ë¥¸ KoBART
    tokenizer="digit82/kobart-summarization",
    max_new_tokens=300,
    device=-1
)


llm = HuggingFacePipeline(pipeline=kobart_pipe)
'''

flan_pipe = pipeline(
    "text2text-generation",
    model="google/flan-t5-xl",
    tokenizer="google/flan-t5-xl",
    max_new_tokens=500,
    device=-1 # apple ì¹© ë‚´ì¥ gpuì—ì„œëŠ” í† ì¹˜ ë¶ˆì•ˆì •í•´ì„œ cpuë¡œ ëŒë¦¬ë„ë¡ ì„¤ì •
)

llm = HuggingFacePipeline(pipeline=flan_pipe)'
'''

# retrieverê°€ ë¬¸ì„œ ê°ì²´ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ, ë¬¸ì„œ ê°ì²´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    prompt
    | llm
    | StrOutputParser()
)
#questionPath = "/Users/soop/s0obang/í•™ë¶€ì—°êµ¬ìƒ24w/RagWithChatbot/questions.txt"
#resultPath = "/Users/soop/s0obang/í•™ë¶€ì—°êµ¬ìƒ24w/RagWithChatbot/results/result1"

#questionPath = r"C:\Soop\ì—°êµ¬\RagTest\ChatBotWithRag\questions.txt"
#resultPath = r"C:\Soop\ì—°êµ¬\RagTest\ChatBotWithRag\results/resultWithOutCustom"

questionPath = "/Users/minseon/2025/í•™ë¶€ì—°êµ¬ìƒ/RAG/RagWithChatbot/questions.txt"
resultPath = "/Users/minseon/2025/í•™ë¶€ì—°êµ¬ìƒ/RAG/RagWithChatbot/results/resultWithOutCustom"

#test(questionPath, resultPath)

query = "ì´ë²ˆ ì¥í•™ê¸ˆ ì„ ê°ë©´ì´ì•¼?"
docs = retriever.invoke(query)
print(f"ğŸ“„ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")
print(f"ğŸ“„ ë¬¸ì„œ ë‚´ìš©: {docs}")

if docs:
    context = format_docs(docs)
    inputs = {"question": query, "context": context}
    answer = rag_chain.invoke(inputs)
    if not answer.strip():
        answer = "ë¬¸ë§¥ì—ì„œ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³¼ë˜ìš”?"
else:
    answer = "âŒ ë¬¸ë§¥ì—ì„œ ì°¸ê³ í•  ë§Œí•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì¤„ë˜ìš”?"

print(f"ì§ˆë¬¸: {query}\n")
print(f"rag ë‹µë³€: {answer}\n")